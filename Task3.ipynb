{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Engineering for Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "#                                   ES335- Machine Learning- Assignment 1\n",
    "#\n",
    "# This file is used to create the dataset for the mini-project. The dataset is created by reading the data from\n",
    "# the Combined folder. The data is then split into training, testing, and validation sets. This split is supposed\n",
    "# to be used for all the modeling purposes.\n",
    "#\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Library imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tsfel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "# Groq API and Models \n",
    "Groq_Token = \"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\"  # Do not share this key with anyone\n",
    "groq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}\n",
    "\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\",\"SITTING\",\"STANDING\",\"WALKING\",\"WALKING_DOWNSTAIRS\",\"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Train Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Test Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Final Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# USE THE BELOW GIVEN DATA FOR TRAINING and TESTING purposes\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train,X_test))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=seed,stratify=y)\n",
    "\n",
    "print(\"Training data shape: \",X_train.shape)\n",
    "print(\"Testing data shape: \",X_test.shape)\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert activity label back to activity name\n",
    "def get_activity_class(activity_number):\n",
    "    activity_classes = {\n",
    "        1: \"WALKING\",\n",
    "        2: \"WALKING_DOWNSTAIRS\",\n",
    "        3: \"WALKING_UPSTAIRS\",\n",
    "        4: \"SITTING\",\n",
    "        5: \"STANDING\",\n",
    "        6: \"LAYING\"\n",
    "    }\n",
    "    \n",
    "    return activity_classes.get(activity_number, \"Invalid input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Learning\n",
    "\n",
    "In Zero-Shot Learning, we are going to start by giving the LLM, the raw acceleration data and ask it to give a prediction without giving it any data for training itself. We have tried two ways in which we are giving it input:\n",
    "\n",
    "    a) Raw Accelerometer Data: In this we are giving the model only the raw accelerometer data in the 3 Axes as input and asking it to predicti the activity class. We have explicitly mentioned in our prompts about the 6 activity class, so it will be easy to use it further for permorfance calculation.\n",
    "\n",
    "    b) TSFEL Data: In this we are creating a more feature-rich input data by using the TSFEL featurizer library. We are applying the feature extraction function to the 'X_train' variable. This changes the shape of the variable from (126, 500, 3) to (126, 1152). But considering that we are going to pass this data to the LLM, we have to take care of token size as well. So after failing to get an output from all the 1152 features, we decided to reduce the number of features to only 400. So the input data now has only the first 400 features that were obtained after using the TSFEL library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Raw Accelerometer Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = []\n",
    "actual_label_list = []\n",
    "\n",
    "\n",
    "for i in range(0,125):\n",
    "    \n",
    "    subject = i\n",
    "\n",
    "    subject_data = X_train[subject]\n",
    "    subject_label = y_train[subject]\n",
    "\n",
    "\n",
    "    # Format the data for the model\n",
    "    prompt = f\"\"\"Here is a sequence of accelerometer data\\nData: {subject_data}\\n\n",
    "    \n",
    "    There are 6 categories of activity that you can choose from namely 'LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS'\n",
    "    What activity class does this data represent?\n",
    "    Don't give explaination, just one-word answer\"\"\"\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0) # type: ignore\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    #print(answer.content)\n",
    "    actual_label = get_activity_class(subject_label)\n",
    "    #print(actual_label)\n",
    "    predicted_label_list.append(answer.content)\n",
    "    actual_label_list.append(actual_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 28.00%\n",
      "Precision: 9.33%\n",
      "Recall: 27.78%\n",
      "F1 Score: 13.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Performance Calculation\n",
    "\n",
    "accuracy = accuracy_score(actual_label_list, predicted_label_list)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "precision = precision_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "\n",
    "recall = recall_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "\n",
    "f1 = f1_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using TSFEL Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='126'\n",
       "                  max='126',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  126\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n"
     ]
    }
   ],
   "source": [
    "#Using the TSFEL library to extract features\n",
    "cfg = tsfel.get_features_by_domain()\n",
    "X_tsfel = tsfel.time_series_features_extractor(cfg, X_train, fs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 1152)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tsfel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = []\n",
    "actual_label_list = []\n",
    "\n",
    "\n",
    "for i in range(0,125):\n",
    "    \n",
    "    subject = i\n",
    "\n",
    "    subject_data = X_tsfel.iloc[subject][:400]\n",
    "    subject_label = y_train[subject]\n",
    "\n",
    "\n",
    "    # Format the data for the model\n",
    "    prompt = f\"\"\"Here is a sequence of Featurized Accelerometer Data \\nData: {subject_data}\\n\n",
    "    \n",
    "    There are 6 categories of activity that you can choose from namely 'LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS'\n",
    "    What activity class does this data represent?\n",
    "    Don't give explaination, just one-word answer\"\"\"\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0) # type: ignore\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    #print(answer.content)\n",
    "    actual_label = get_activity_class(subject_label)\n",
    "    #print(actual_label)\n",
    "    predicted_label_list.append(answer.content)\n",
    "    actual_label_list.append(actual_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.40%\n",
      "Precision: 11.49%\n",
      "Recall: 22.22%\n",
      "F1 Score: 11.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Performance Calculation\n",
    "\n",
    "accuracy = accuracy_score(actual_label_list, predicted_label_list)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "precision = precision_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "\n",
    "recall = recall_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "\n",
    "f1 = f1_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Learning\n",
    "\n",
    "In case of Few-shot learning, we need to give the LLM some prior data, using which it can train itself and then give a data that it has never seen before for prediction. Similar to Zero-Shot, we have tried to give the data in two ways. But before that we need to pre-process the labels. Since, we are going to give the LLM all the 'X_train' and 'y_train' data directly, changing the shape of 'y_train' may help the model to understand the context of data a bit better. Also it helps pass the labels of a particular instance inside loop quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_expanded = np.expand_dims(y_train, axis=-1)  # Shape becomes (126, 1)\n",
    "y_train_expanded = np.expand_dims(y_train_expanded, axis=-1)  # Shape becomes (126, 1, 1)\n",
    "y_train_broadcasted = np.tile(y_train_expanded, (1, 500, 1))  # Shape becomes (126, 500, 1)\n",
    "\n",
    "new_y_train = y_train_broadcasted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Raw accelerometer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = []\n",
    "actual_label_list = []\n",
    "\n",
    "for i in range(1, 54):\n",
    "    \n",
    "    test_sub = i\n",
    "\n",
    "    train_sub_data = X_train\n",
    "    train_sub_label = new_y_train\n",
    "\n",
    "    test_sub_data = X_test[test_sub]\n",
    "    test_sub_label = get_activity_class(y_test[test_sub])\n",
    "\n",
    "    # Format the data for the model\n",
    "    prompt = f\"\"\"Here is a sequence of accelerometer data and corresponding activity label:\\nData: {train_sub_data}\\nLabel: {train_sub_label}\\n\n",
    "    \n",
    "    The data that is of shape (126, 500, 3) which means there are 126 instances of 500 data points in each of the 3 axes.\n",
    "    The label is the corresponding activity the human is currently doing.\n",
    "    Shape of label is (126, 500, 1) which means for each instance there is only 1 label.\n",
    "\n",
    "    The next sequence of accelerometer data is this: \\nData: {test_sub_data}\\n\n",
    "    Your job is to predict the activity class of this data using the data and corresponding label that we gave earlier\n",
    "    There are 6 categories of activity that you can choose from namely 'LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS'\n",
    "    What activity class does this data represent?\n",
    "    Give one-word answer\"\"\"\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0) # type: ignore\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    predicted_label_list.append(answer.content)\n",
    "    actual_label_list.append(test_sub_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 28.30%\n",
      "Precision: 9.55%\n",
      "Recall: 27.78%\n",
      "F1 Score: 14.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Performance Calculation\n",
    "\n",
    "accuracy = accuracy_score(actual_label_list, predicted_label_list)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "precision = precision_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "\n",
    "recall = recall_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "\n",
    "f1 = f1_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using TSFEL Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = []\n",
    "actual_label_list = []\n",
    "\n",
    "for i in range(0, 52):\n",
    "    \n",
    "    test_sub = i\n",
    "\n",
    "    train_sub_data = X_tsfel.iloc[:, :400]\n",
    "    train_sub_label = y_train\n",
    "\n",
    "    test_sub_data = X_test[test_sub]\n",
    "    test_sub_label = get_activity_class(y_test[test_sub])\n",
    "\n",
    "    # Format the data for the model\n",
    "    prompt = f\"\"\"Here is a sequence of featurized accelerometer data and corresponding activity label:\\nData: {train_sub_data}\\nLabel: {train_sub_label}\\n\n",
    "    \n",
    "    The data is of shape (126 , 400) which means for each of the 126 subjects, there are 400 features.\n",
    "    The label is the activity the human is currently doing corresponding to the given data.\n",
    "\n",
    "    The next sequence of accelerometer data is this: \\nData: {test_sub_data}\\n\n",
    "    Your job is to predict the activity class of this data using the data and corresponding label that we gave earlier\n",
    "    There are 6 categories of activity that you can choose from namely 'LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS'\n",
    "    What activity class does this data represent?\n",
    "    Give only one-word answer\"\"\"\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0) # type: ignore\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    predicted_label_list.append(answer.content)\n",
    "    actual_label_list.append(test_sub_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 28.85%\n",
      "Precision: 7.39%\n",
      "Recall: 18.52%\n",
      "F1 Score: 10.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\abhyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Performance Calculation\n",
    "\n",
    "accuracy = accuracy_score(actual_label_list, predicted_label_list)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "precision = precision_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "\n",
    "recall = recall_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "\n",
    "f1 = f1_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment: Changing the labels to Text\n",
    "\n",
    "We tried to change the labels from numeric to textual data. Since we are giving these inputs to LLM, it seems logical to have more text data so that it can understand the context much better. But the result which we saw was completely opposite. The accuracy dropped by using text labels instead of numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label = []\n",
    "for i in range(0,52):\n",
    "    y_train_label.append(get_activity_class(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = []\n",
    "actual_label_list = []\n",
    "\n",
    "for i in range(0, 52):\n",
    "    \n",
    "    test_sub = i\n",
    "\n",
    "    train_sub_data = X_tsfel.iloc[:, :400]\n",
    "    train_sub_label = y_train_label\n",
    "\n",
    "    test_sub_data = X_test[test_sub]\n",
    "    test_sub_label = get_activity_class(y_test[test_sub])\n",
    "\n",
    "    # Format the data for the model\n",
    "    prompt = f\"\"\"Here is a sequence of featurized accelerometer data and corresponding activity label:\\nData: {train_sub_data}\\nLabel: {train_sub_label}\\n\n",
    "    \n",
    "    The data is of shape (126 , 400) which means for each of the 126 subjects, there are 400 features.\n",
    "    The label is the activity the human is currently doing corresponding to the given data.\n",
    "\n",
    "    The next sequence of accelerometer data is this: \\nData: {test_sub_data}\\n\n",
    "    Your job is to predict the activity class of this data using the data and corresponding label that we gave earlier\n",
    "    There are 6 categories of activity that you can choose from namely 'LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS'\n",
    "    What activity class does this data represent?\n",
    "    Give only one-word answer\"\"\"\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0) # type: ignore\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    predicted_label_list.append(answer.content)\n",
    "    actual_label_list.append(test_sub_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.23%\n",
      "Precision: 11.33%\n",
      "Recall: 18.52%\n",
      "F1 Score: 8.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_label_list, predicted_label_list)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(actual_label_list, predicted_label_list, average='macro')\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "##### Performance Analysis:\n",
    "\n",
    "The performance metrics of each of the tests are listed below:\n",
    "\n",
    "a) Zero-Shot with Raw data: \n",
    "- Accuracy: 28.00%\n",
    "- Precision: 9.33%\n",
    "- Recall: 27.78%\n",
    "- F1 Score: 13.97%\n",
    "\n",
    "b) Zero-Shot with TSFEL features:\n",
    "- Accuracy: 22.40%\n",
    "- Precision: 11.49%\n",
    "- Recall: 22.22%\n",
    "- F1 Score: 11.97%\n",
    "\n",
    "c) Few-Shot with Raw data:\n",
    "- Accuracy: 28.30%\n",
    "- Precision: 9.55%\n",
    "- Recall: 27.78%\n",
    "- F1 Score: 14.14%\n",
    "\n",
    "d) Few-Shot with TSFEL features:\n",
    "- Accuracy: 28.85%\n",
    "- Precision: 7.39%\n",
    "- Recall: 18.52%\n",
    "- F1 Score: 10.21%\n",
    "\n",
    "**Conclusion:** We can observe that the performance using only the raw data is coming out to be better in both Zero-shot and Few-shot. Performance difference between Zero-shot and Few-shot is almost negligible. Both are performing similarly.\n",
    "\n",
    "**Important Observation:** \n",
    "- While performing the task, it was observed that the model was predicting all the given data as either WALKING or STANDING. \n",
    "- On further investigating the outputs given by the LLM, it was observed that it was only focusing on the magnitude of the features to predict the activity. For e.g., when a data showed that the magnitude of acceleration is relatively high, it thinks that the activity is some kind of dynamic activity and by-default thinks that it must be WALKING.\n",
    "- Similarly, when a given data has relatively smaller values, the model thinks that the activity resembles a static activity and by-default it predicts the activity as STANDING.\n",
    "- This could be the reason, that even in Few-shot learning , the model isn't trying to learn the patterns from the data given by us. It is just predicting the activity using its own interpretitions and hence the lower performance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "##### Performance against Decision Tree:\n",
    "\n",
    "a) Few-Shot with Raw data:\n",
    "- Accuracy: 28.30%\n",
    "- Precision: 9.55%\n",
    "- Recall: 27.78%\n",
    "- F1 Score: 14.14%\n",
    "\n",
    "b) Few-Shot with TSFEL features:\n",
    "- Accuracy: 28.85%\n",
    "- Precision: 7.39%\n",
    "- Recall: 18.52%\n",
    "- F1 Score: 10.21%\n",
    "\n",
    "c) Decision tree with Raw data:\n",
    "- Accuracy: 61%\n",
    "- Precision: 56%\n",
    "- Recall: 61%\n",
    "- F1 Score: 58%\n",
    "\n",
    "d) Decision tree with TSFEL features:\n",
    "- Accuracy: 89%\n",
    "- Precision: 89%\n",
    "- Recall: 89%\n",
    "- F1 Score: 89%\n",
    "\n",
    "\n",
    "**Conclusion:** \n",
    "- Decision Tree is performing much better as compared to Few-Shot learning. \n",
    "- This is due the reason discussed in the previous question i.e., the LLM model is not actually learning any patterns by the data provided by us. It is just predicting the activity using the magnitude of given features. If the activity resembles a dynamic activity it defaults to WALKING and similarly, if the activity resembles a static activity, it defaults to STANDING.\n",
    "- Decision Tree, on the other hand, is actually learning using the data and then giving the predictions accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "##### Limitations:\n",
    "Some of the limitations of Zero-Shot ond Few-Shot Learning that we observed were:\n",
    "- Performance of the Zero-Shot and Few-Shot Learning is nowhere near the performance of decision trees. Decision Tree was performing much better.\n",
    "- Generating the output using LLM also takes a lot of time some of the scripts ran for more than 15 min. Whereas, Decision tree was much more efficient and also giving better accuracy.\n",
    "- The LLM was not showing any signs of learning from the given data in case of Few-Shot learning. It was just defaulting to two values.\n",
    "- The size of data which can be given as input is also a bottleneck in case of Zero-Shot and Few-Shot learning. The LLM model that we used i.e., LLaMa3-70b has a token limit of 6000. Decision Tree was easily trained on the TSFEL features (1152 Features), but when the same data was given to LLaMa3, it was hitting the token limit. So we had to reduce the size of data which we were passing to the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "For this task, we gave the LLM model only the first 16 instances of 'X_train' which didn't contain data of 'WALKING_UPSTAIRS' class. The data and the corresponding labels were given to train the LLM and then from the 'X_test' we chose some indices which contains only those data which belong to the 'WALKING_UPSTAIRS' class.\n",
    "\n",
    "**Observations:** \n",
    "\n",
    "- The model was predicting WALKING_UPSTAIRS as either WALKING or STANDING.\n",
    "- The observations are in-line with what we have observed so far in this Task i.e., the LLM is defaulting to only 2 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = []\n",
    "actual_label_list = []\n",
    "index = [0, 8, 21, 34, 37]\n",
    "\n",
    "for i in index:\n",
    "    \n",
    "    test_sub = i\n",
    "\n",
    "    train_sub_data = X_train[:16]\n",
    "    train_sub_label = new_y_train[:16]\n",
    "\n",
    "    test_sub_data = X_test[test_sub]\n",
    "    test_sub_label = get_activity_class(y_test[test_sub])\n",
    "\n",
    "    # Format the data for the model\n",
    "    prompt = f\"\"\"Here is a sequence of accelerometer data and corresponding activity label:\\nData: {train_sub_data}\\nLabel: {train_sub_label}\\n\n",
    "    \n",
    "    The data that is of shape (16, 500, 3) which means there are 16 instances of 500 data points in each of the 3 axes.\n",
    "    The label is the corresponding activity the human is currently doing.\n",
    "    Shape of label is (16, 500, 1) which means for each instance there is only 1 label.\n",
    "\n",
    "    The next sequence of accelerometer data is this: \\nData: {test_sub_data}\\n\n",
    "    Your job is to predict the activity class of this data using the data and corresponding label that we gave earlier\n",
    "    There are 6 categories of activity that you can choose from namely 'LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS'\n",
    "    What activity class does this data represent?\n",
    "    Give one-word answer\"\"\"\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0) # type: ignore\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    predicted_label_list.append(answer.content)\n",
    "    actual_label_list.append(test_sub_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STANDING', 'WALKING', 'WALKING', 'STANDING', 'WALKING']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WALKING_UPSTAIRS',\n",
       " 'WALKING_UPSTAIRS',\n",
       " 'WALKING_UPSTAIRS',\n",
       " 'WALKING_UPSTAIRS',\n",
       " 'WALKING_UPSTAIRS']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#Making 10 instances of random data\n",
    "instances = 10\n",
    "datapoints = 500\n",
    "axes = 3\n",
    "\n",
    "# Random accelerometer data\n",
    "data = np.random.uniform(low=-10, high=10, size=(instances, datapoints, axes))\n",
    "\n",
    "print(data.shape)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "In this task, we have generated a random dataset of shape (10, 500, 3) which means 10 instances of 500 datapoints in each of the 3 axes. We applied Zero-shot learning for this task because applying Few-shot doesn't seem to show any benefit from our experiments. After applying the Zero-Shot, we can observe the same pattern, that the LLM is just defaulting to classify it as WALKING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WALKING\n",
      "WALKING\n",
      "WALKING\n",
      "WALKING\n",
      "WALKING\n",
      "WALKING\n",
      "WALKING\n",
      "WALKING\n",
      "WALKING\n"
     ]
    }
   ],
   "source": [
    "predicted_label_list = []\n",
    "actual_label_list = []\n",
    "\n",
    "\n",
    "for i in range(0,9):\n",
    "    \n",
    "    subject = i\n",
    "\n",
    "    subject_data = data[subject]\n",
    "\n",
    "\n",
    "    # Format the data for the model\n",
    "    prompt = f\"\"\"Here is a sequence of accelerometer data\\nData: {subject_data}\\n\n",
    "    \n",
    "    There are 6 categories of activity that you can choose from namely 'LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS'\n",
    "    What activity class does this data represent?\n",
    "    Don't give explaination, just one-word answer\"\"\"\n",
    "\n",
    "    # To use Groq LLMs \n",
    "    model_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0) # type: ignore\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    print(answer.content)\n",
    "    #actual_label = get_activity_class(subject_label)\n",
    "    #print(actual_label)\n",
    "    predicted_label_list.append(answer.content)\n",
    "    #actual_label_list.append(actual_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
